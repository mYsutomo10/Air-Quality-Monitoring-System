{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup dan Import\n",
    "!pip install -q google-cloud-aiplatform\n",
    "!pip install -q google-cloud-storage\n",
    "!pip install -q google-cloud-bigquery\n",
    "!pip install -q pandas numpy scipy scikit-learn\n",
    "!pip install -q matplotlib seaborn\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup project\n",
    "PROJECT_ID = \"your-project-id\"\n",
    "BUCKET = \"your-bucket\"\n",
    "REGION = \"your-region\"\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from Cloud Storage\n",
    "def load_data_from_gcs(bucket_name, blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    # Download as string\n",
    "    data_str = blob.download_as_string()\n",
    "    \n",
    "    # Parse CSV\n",
    "    return pd.read_csv(pd.StringIO(data_str.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load sensor dan reference data\n",
    "sensor_data = load_data_from_gcs(BUCKET, 'raw/sensor_data.csv')\n",
    "reference_data = load_data_from_gcs(BUCKET, 'raw/reference_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data Exploration\n",
    "def explore_data(df, title):\n",
    "    print(f\"\\n{title} Data Exploration\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nShape:\", df.shape)\n",
    "    print(\"\\nColumns:\", df.columns.tolist())\n",
    "    print(\"\\nData Types:\\n\", df.dtypes)\n",
    "    print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
    "    print(\"\\nSummary Statistics:\\n\", df.describe())\n",
    "\n",
    "explore_data(sensor_data, \"Sensor\")\n",
    "explore_data(reference_data, \"Reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "def preprocess_data(sensor_df, reference_df):\n",
    "    # Convert timestamps\n",
    "    sensor_df['timestamp'] = pd.to_datetime(sensor_df['timestamp'])\n",
    "    reference_df['timestamp'] = pd.to_datetime(reference_df['timestamp'])\n",
    "    \n",
    "    # Remove invalid values\n",
    "    sensor_df = sensor_df[sensor_df['pm25'] >= 0]\n",
    "    sensor_df = sensor_df[sensor_df['pm10'] >= 0]\n",
    "    sensor_df = sensor_df[sensor_df['o3'] >= 0]\n",
    "    sensor_df = sensor_df[sensor_df['co'] >= 0]\n",
    "    sensor_df = sensor_df[sensor_df['no2'] >= 0]\n",
    "    \n",
    "    # Match timestamps between sensor and reference data\n",
    "    merged_df = pd.merge(sensor_df, reference_df, \n",
    "                        on='timestamp', \n",
    "                        suffixes=('_sensor', '_reference'))\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "processed_data = preprocess_data(sensor_data, reference_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "def plot_correlations(df, parameters):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, param in enumerate(parameters):\n",
    "        sensor_col = f\"{param}_sensor\"\n",
    "        ref_col = f\"{param}_reference\"\n",
    "        \n",
    "        sns.scatterplot(data=df, x=sensor_col, y=ref_col, ax=axes[idx])\n",
    "        axes[idx].set_title(f'{param} Correlation')\n",
    "        \n",
    "        # Add correlation coefficient\n",
    "        corr = df[sensor_col].corr(df[ref_col])\n",
    "        axes[idx].text(0.05, 0.95, f'r = {corr:.2f}', \n",
    "                      transform=axes[idx].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "parameters = ['pm25', 'pm10', 'o3', 'co', 'no2']\n",
    "plot_correlations(processed_data, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "def save_to_gcs(df, bucket_name, blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    # Save to CSV\n",
    "    blob.upload_from_string(df.to_csv(index=False))\n",
    "\n",
    "save_to_gcs(processed_data, BUCKET, 'processed/training_data.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
