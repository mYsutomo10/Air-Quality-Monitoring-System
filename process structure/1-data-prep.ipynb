{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup dan Import\n",
    "!pip install -q google-cloud-aiplatform\n",
    "!pip install -q google-cloud-storage\n",
    "!pip install -q pandas numpy scipy scikit-learn\n",
    "!pip install -q matplotlib seaborn\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup project\n",
    "PROJECT_ID = \"your-project-id\"\n",
    "BUCKET = \"your-bucket\"\n",
    "REGION = \"your-region\"\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from Cloud Storage\n",
    "def load_data_from_gcs(bucket_name, blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    # Download as string\n",
    "    data_str = blob.download_as_string()\n",
    "    \n",
    "    # Parse CSV\n",
    "    return pd.read_csv(pd.StringIO(data_str.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load sensor, reference data, weather data\n",
    "sensor_data = load_data_from_gcs(BUCKET, 'raw/sensor_data.csv')\n",
    "reference_data = load_data_from_gcs(BUCKET, 'raw/reference_data.csv')\n",
    "weather_data = load_data_from_gcs(BUCKET, 'raw/weather_api_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data Exploration\n",
    "def explore_data(df, title):\n",
    "    print(f\"\\n{title} Data Exploration\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nShape:\", df.shape)\n",
    "    print(\"\\nColumns:\", df.columns.tolist())\n",
    "    print(\"\\nData Types:\\n\", df.dtypes)\n",
    "    print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
    "    print(\"\\nSummary Statistics:\\n\", df.describe())\n",
    "\n",
    "explore_data(sensor_data, \"Sensor\")\n",
    "explore_data(reference_data, \"Reference\")\n",
    "explore_data(weather_data, \"Weather API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing Weather Data\n",
    "def preprocess_weather_data(weather_df):\n",
    "    # Convert timestamps\n",
    "    weather_df['timestamp'] = pd.to_datetime(weather_df['timestamp'])\n",
    "    \n",
    "    # Rename columns untuk konsistensi (jika diperlukan)\n",
    "    # Contoh: jika kolom di weather API berbeda namanya\n",
    "    column_mapping = {\n",
    "        'wind_direction': 'wind_direction',\n",
    "        'wind_speed': 'wind_speed',\n",
    "        'temperature': 'temperature',\n",
    "        'precipitation': 'precipitation'\n",
    "    }\n",
    "    \n",
    "    weather_df = weather_df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Handle missing values if any\n",
    "    weather_df['wind_direction'].fillna(weather_df['wind_direction'].median(), inplace=True)\n",
    "    weather_df['wind_speed'].fillna(weather_df['wind_speed'].median(), inplace=True)\n",
    "    weather_df['temperature'].fillna(weather_df['temperature'].median(), inplace=True)\n",
    "    weather_df['precipitation'].fillna(0, inplace=True)  # Asumsi 0 untuk data curah hujan yang hilang\n",
    "    \n",
    "    return weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "def preprocess_data(sensor_df, reference_df, weather_df):\n",
    "    # Convert timestamps\n",
    "    sensor_df['timestamp'] = pd.to_datetime(sensor_df['timestamp'])\n",
    "    reference_df['timestamp'] = pd.to_datetime(reference_df['timestamp'])\n",
    "    weather_df['timestamp'] = pd.to_datetime(weather_df['timestamp'])\n",
    "    \n",
    "    # Remove invalid values\n",
    "    sensor_df = sensor_df[sensor_df['pm25'] >= 0]\n",
    "    sensor_df = sensor_df[sensor_df['pm10'] >= 0]\n",
    "    sensor_df = sensor_df[sensor_df['o3'] >= 0]\n",
    "    sensor_df = sensor_df[sensor_df['co'] >= 0]\n",
    "    sensor_df = sensor_df[sensor_df['no2'] >= 0]\n",
    "    \n",
    "    # Match timestamps between sensor and reference data first\n",
    "    merged_df = pd.merge(sensor_df, reference_df, \n",
    "                       on='timestamp', \n",
    "                       suffixes=('_sensor', '_reference'))\n",
    "    \n",
    "    # Now merge with weather data\n",
    "    final_df = pd.merge(merged_df, weather_df, on='timestamp', how='left')\n",
    "    \n",
    "    # Handle any timestamp mismatches with weather data using forward fill\n",
    "    # (asumsi data weather mungkin interval waktunya berbeda)\n",
    "    final_df['wind_direction'].fillna(method='ffill', inplace=True)\n",
    "    final_df['wind_speed'].fillna(method='ffill', inplace=True)\n",
    "    final_df['temperature'].fillna(method='ffill', inplace=True)\n",
    "    final_df['precipitation'].fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess weather data\n",
    "processed_weather_data = preprocess_weather_data(weather_data)\n",
    "\n",
    "# Merge all datasets\n",
    "processed_data = preprocess_data(sensor_data, reference_data, processed_weather_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization - correlation between pollution and weather\n",
    "def plot_weather_correlations(df, pollutants, weather_params):\n",
    "    # Set up a grid for the correlation plots\n",
    "    rows = len(pollutants)\n",
    "    cols = len(weather_params)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*3))\n",
    "    \n",
    "    # If we only have one row, reshape axes for consistent indexing\n",
    "    if rows == 1:\n",
    "        axes = np.array([axes])\n",
    "    \n",
    "    # If we only have one pollutant and one weather param, ensure axes is 2D\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    \n",
    "    for i, pollutant in enumerate(pollutants):\n",
    "        for j, weather_param in enumerate(weather_params):\n",
    "            sns.scatterplot(data=df, x=weather_param, y=f\"{pollutant}_sensor\", ax=axes[i, j])\n",
    "            axes[i, j].set_title(f'{pollutant} vs {weather_param}')\n",
    "            \n",
    "            # Add correlation coefficient\n",
    "            corr = df[f\"{pollutant}_sensor\"].corr(df[weather_param])\n",
    "            axes[i, j].text(0.05, 0.95, f'r = {corr:.2f}', transform=axes[i, j].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Extended visualization untuk wind direction rose plot\n",
    "def plot_wind_rose(df, pollutant):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Group by wind direction and calculate mean pollutant value\n",
    "    wind_groups = df.groupby(pd.cut(df['wind_direction'], bins=8, labels=False))\n",
    "    mean_pollutant = wind_groups[f'{pollutant}_sensor'].mean()\n",
    "    \n",
    "    # Create polar plot\n",
    "    ax = plt.subplot(111, projection='polar')\n",
    "    \n",
    "    # Convert wind direction to radians\n",
    "    theta = np.linspace(0, 2*np.pi, 8, endpoint=False)\n",
    "    \n",
    "    # Plot as bars in polar coordinates\n",
    "    bars = ax.bar(theta, mean_pollutant, width=2*np.pi/8, bottom=0.0)\n",
    "    \n",
    "    # Set the direction labels\n",
    "    ax.set_xticks(theta)\n",
    "    ax.set_xticklabels(['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW'])\n",
    "    \n",
    "    # Set title\n",
    "    ax.set_title(f'Mean {pollutant} by Wind Direction', y=1.08)\n",
    "    \n",
    "    # Colorize based on value\n",
    "    min_val = mean_pollutant.min()\n",
    "    max_val = mean_pollutant.max()\n",
    "    norm = plt.Normalize(min_val, max_val)\n",
    "    colors = plt.cm.viridis(norm(mean_pollutant))\n",
    "    \n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_facecolor(color)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot correlations\n",
    "pollutant_params = ['pm25', 'pm10', 'o3', 'co', 'no2']\n",
    "weather_params = ['wind_direction', 'wind_speed', 'temperature', 'precipitation']\n",
    "\n",
    "plot_weather_correlations(processed_data, pollutant_params, weather_params)\n",
    "\n",
    "# Plot wind rose for PM2.5\n",
    "plot_wind_rose(processed_data, 'pm25')\n",
    "\n",
    "# Correlation heatmap of all variables\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Select only the relevant columns\n",
    "corr_columns = [col for col in processed_data.columns \n",
    "                if any(param in col for param in pollutant_params) or \n",
    "                col in weather_params]\n",
    "                \n",
    "corr_matrix = processed_data[corr_columns].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap between Pollutants and Weather Parameters')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Persiapan Time Series untuk model prediksi\n",
    "def prepare_time_series_features(df):\n",
    "    \"\"\"\n",
    "    Mempersiapkan fitur time series dari dataset untuk keperluan prediksi.\n",
    "    \"\"\"\n",
    "    # Pastikan dataframe terurut berdasarkan timestamp\n",
    "    df = df.sort_values('timestamp')\n",
    "    \n",
    "    # Ekstrak fitur waktu\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day'] = df['timestamp'].dt.day\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek  # 0=Senin, 6=Minggu\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    \n",
    "    # Fitur siklikal untuk variabel siklis (jam, hari dalam seminggu)\n",
    "    # Mengkonversi variabel siklis ke representasi sinus dan cosinus\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week']/7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week']/7)\n",
    "    \n",
    "    # Lag features (contoh untuk PM2.5)\n",
    "    df['pm25_sensor_lag1h'] = df['pm25_sensor'].shift(1)\n",
    "    df['pm25_sensor_lag3h'] = df['pm25_sensor'].shift(3)\n",
    "    df['pm25_sensor_lag6h'] = df['pm25_sensor'].shift(6)\n",
    "    df['pm25_sensor_lag12h'] = df['pm25_sensor'].shift(12)\n",
    "    df['pm25_sensor_lag24h'] = df['pm25_sensor'].shift(24)\n",
    "    \n",
    "    # Rolling windows untuk mendapatkan tren (contoh untuk PM2.5)\n",
    "    df['pm25_sensor_rolling_mean_3h'] = df['pm25_sensor'].rolling(window=3).mean()\n",
    "    df['pm25_sensor_rolling_mean_6h'] = df['pm25_sensor'].rolling(window=6).mean()\n",
    "    df['pm25_sensor_rolling_mean_12h'] = df['pm25_sensor'].rolling(window=12).mean()\n",
    "    df['pm25_sensor_rolling_std_3h'] = df['pm25_sensor'].rolling(window=3).std()\n",
    "    df['pm25_sensor_rolling_std_12h'] = df['pm25_sensor'].rolling(window=12).std()\n",
    "    \n",
    "    # Tambahkan lag features untuk parameter cuaca\n",
    "    df['wind_speed_lag3h'] = df['wind_speed'].shift(3)\n",
    "    df['temperature_lag3h'] = df['temperature'].shift(3)\n",
    "    \n",
    "    # Deteksi tren (slope) menggunakan diferensiasi\n",
    "    df['pm25_sensor_diff_1h'] = df['pm25_sensor'].diff(1)\n",
    "    df['pm25_sensor_diff_3h'] = df['pm25_sensor'].diff(3)\n",
    "    \n",
    "    # Menangani nilai null yang dihasilkan dari operasi shift dan rolling\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(method='bfill').fillna(method='ffill')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Persiapan data untuk cross-validation time series\n",
    "def prepare_time_series_splits(df, target_col, n_splits=5):\n",
    "    \"\"\"\n",
    "    Menyiapkan indeks untuk cross-validation time series\n",
    "    yang menghormati urutan kronologis data.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    \n",
    "    # Buat TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    # Simpan indeks split dalam dataframe\n",
    "    splits = []\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(df):\n",
    "        splits.append({\n",
    "            'train_start': df.iloc[train_idx[0]]['timestamp'],\n",
    "            'train_end': df.iloc[train_idx[-1]]['timestamp'],\n",
    "            'test_start': df.iloc[test_idx[0]]['timestamp'],\n",
    "            'test_end': df.iloc[test_idx[-1]]['timestamp'],\n",
    "            'train_idx': train_idx,\n",
    "            'test_idx': test_idx\n",
    "        })\n",
    "    \n",
    "    # Simpan informasi split untuk digunakan di model development\n",
    "    split_df = pd.DataFrame(splits)\n",
    "    \n",
    "    return split_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Terapkan time series features\n",
    "processed_data = prepare_time_series_features(processed_data)\n",
    "\n",
    "# Menyiapkan informasi split untuk time series cross-validation\n",
    "# (asumsi target prediksi adalah pm25_sensor)\n",
    "splits_info = prepare_time_series_splits(processed_data, 'pm25_sensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualisasi distribusi fitur time series\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "processed_data.groupby('hour')['pm25_sensor'].mean().plot(kind='bar')\n",
    "plt.title('Mean PM2.5 by Hour of Day')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "processed_data.groupby('day_of_week')['pm25_sensor'].mean().plot(kind='bar')\n",
    "plt.title('Mean PM2.5 by Day of Week')\n",
    "plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "processed_data.groupby('month')['pm25_sensor'].mean().plot(kind='bar')\n",
    "plt.title('Mean PM2.5 by Month')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.boxplot(data=processed_data, x='is_weekend', y='pm25_sensor')\n",
    "plt.title('PM2.5 Distribution: Weekday vs Weekend')\n",
    "plt.xticks([0, 1], ['Weekday', 'Weekend'])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.scatter(processed_data['wind_speed'], processed_data['pm25_sensor'], alpha=0.5)\n",
    "plt.title('PM2.5 vs Wind Speed')\n",
    "plt.xlabel('Wind Speed')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "processed_data['pm25_sensor'].plot(kind='hist', bins=30)\n",
    "plt.title('PM2.5 Distribution')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot time series untuk PM2.5\n",
    "plt.plot(processed_data['timestamp'], processed_data['pm25_sensor'])\n",
    "plt.title('PM2.5 Time Series')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('PM2.5 Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Simpan data time series yang sudah diproses\n",
    "save_to_gcs(processed_data, BUCKET, 'processed/time_series_training_data.csv')\n",
    "\n",
    "# Simpan informasi split untuk digunakan pada model development\n",
    "save_to_gcs(splits_info, BUCKET, 'processed/time_series_splits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save processed data including weather parameters\n",
    "def save_to_gcs(df, bucket_name, blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    \n",
    "    # Save to CSV\n",
    "    blob.upload_from_string(df.to_csv(index=False))\n",
    "\n",
    "save_to_gcs(processed_data, BUCKET, 'processed/training_data_with_weather.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
