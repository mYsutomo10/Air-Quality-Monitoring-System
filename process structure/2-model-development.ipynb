{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup dan Import\n",
    "!pip install -q google-cloud-aiplatform\n",
    "!pip install -q google-cloud-storage\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q pandas numpy matplotlib seaborn joblib\n",
    "!pip install -q statsmodels pmdarima xgboost lightgbm\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima\n",
    "from datetime import timedelta\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup project\n",
    "PROJECT_ID = \"your-project-id\"\n",
    "BUCKET = \"your-bucket\"\n",
    "REGION = \"your-region\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "def load_data_from_gcs(bucket_name, blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    data_str = blob.download_as_string()\n",
    "    return pd.read_csv(pd.StringIO(data_str.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the time series data prepared in notebook 1\n",
    "processed_data = load_data_from_gcs(BUCKET, 'processed/training_data_with_weather.csv')\n",
    "splits_info = load_data_from_gcs(BUCKET, 'processed/time_series_splits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "processed_data['timestamp'] = pd.to_datetime(processed_data['timestamp'])\n",
    "\n",
    "# Make sure data is sorted by timestamp\n",
    "processed_data = processed_data.sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Base Model Class for Calibration\n",
    "class AirQualityCalibrator:\n",
    "    def __init__(self):\n",
    "        self.parameters = ['pm25', 'pm10', 'o3', 'co', 'no2']\n",
    "        self.models = {}\n",
    "        \n",
    "    def prepare_features(self, data):\n",
    "        # Include weather parameters based on notebook 1\n",
    "        features = data[[\n",
    "            'pm25_sensor', 'pm10_sensor', 'o3_sensor', \n",
    "            'co_sensor', 'no2_sensor', 'temperature', 'humidity',\n",
    "            'wind_direction', 'wind_speed', 'precipitation'\n",
    "        ]]\n",
    "        return features\n",
    "    \n",
    "    def train(self, X_train, y_train, param):\n",
    "        print(f\"Training model for {param}...\")\n",
    "        \n",
    "        # Define parameter grid for GridSearchCV\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "        \n",
    "        # Initialize base model\n",
    "        base_model = RandomForestRegressor(random_state=42)\n",
    "        \n",
    "        # Perform GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            base_model, param_grid, cv=5, \n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"Best parameters for {param}: {grid_search.best_params_}\")\n",
    "        self.models[param] = grid_search.best_estimator_\n",
    "        \n",
    "        # Get feature importances\n",
    "        feature_importances = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance': grid_search.best_estimator_.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 5 important features:\")\n",
    "        print(feature_importances.head())\n",
    "        \n",
    "        return feature_importances\n",
    "        \n",
    "    def evaluate(self, X_test, y_test, param):\n",
    "        predictions = self.models[param].predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        \n",
    "        # Handle zeros in y_test to avoid division by zero in MAPE\n",
    "        mape = mean_absolute_percentage_error(y_test, predictions) * 100\n",
    "        \n",
    "        return {\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'predictions': predictions\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Time Series Model Class\n",
    "class TimeSeriesForecaster:\n",
    "    def __init__(self, target_parameter='pm25'):\n",
    "        self.target_parameter = target_parameter\n",
    "        self.sarimax_model = None\n",
    "        self.xgb_model = None\n",
    "        self.lgb_model = None\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_features(self, df, target_col):\n",
    "        \"\"\"\n",
    "        Prepare features for time series forecasting.\n",
    "        Uses the time series features created in notebook 1.\n",
    "        \"\"\"\n",
    "        # Select all time-related features and other relevant features\n",
    "        feature_cols = [\n",
    "            # Time features\n",
    "            'hour', 'day', 'day_of_week', 'month', 'is_weekend',\n",
    "            'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
    "            \n",
    "            # Lag features\n",
    "            f'{target_col}_lag1h', f'{target_col}_lag3h', f'{target_col}_lag6h', \n",
    "            f'{target_col}_lag12h', f'{target_col}_lag24h',\n",
    "            \n",
    "            # Rolling window features\n",
    "            f'{target_col}_rolling_mean_3h', f'{target_col}_rolling_mean_6h',\n",
    "            f'{target_col}_rolling_mean_12h', f'{target_col}_rolling_std_3h',\n",
    "            f'{target_col}_rolling_std_12h',\n",
    "            \n",
    "            # Trend features\n",
    "            f'{target_col}_diff_1h', f'{target_col}_diff_3h',\n",
    "            \n",
    "            # Weather features\n",
    "            'wind_direction', 'wind_speed', 'temperature', 'precipitation',\n",
    "            'wind_speed_lag3h', 'temperature_lag3h'\n",
    "        ]\n",
    "        \n",
    "        # Drop rows with NaN values which can occur after creating lag features\n",
    "        features = df[feature_cols].dropna()\n",
    "        return features\n",
    "    \n",
    "    def train_sarimax(self, train_data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 24)):\n",
    "        \"\"\"\n",
    "        Train a SARIMAX model for time series forecasting.\n",
    "        \"\"\"\n",
    "        print(f\"Training SARIMAX model for {self.target_parameter}...\")\n",
    "        \n",
    "        # If auto_arima is True, automatically find the best parameters\n",
    "        if order == 'auto':\n",
    "            print(\"Running auto_arima to find best parameters...\")\n",
    "            auto_model = auto_arima(\n",
    "                train_data,\n",
    "                seasonal=True,\n",
    "                m=24,  # Daily seasonality (24 hours)\n",
    "                d=1,\n",
    "                D=1,\n",
    "                start_p=0, start_q=0,\n",
    "                max_p=3, max_q=3,\n",
    "                max_P=2, max_Q=2,\n",
    "                max_d=2, max_D=1,\n",
    "                trace=True,\n",
    "                error_action='ignore',\n",
    "                suppress_warnings=True,\n",
    "                stepwise=True\n",
    "            )\n",
    "            print(f\"Best SARIMAX parameters: {auto_model.order}, {auto_model.seasonal_order}\")\n",
    "            order = auto_model.order\n",
    "            seasonal_order = auto_model.seasonal_order\n",
    "        \n",
    "        # Train SARIMAX model with given parameters\n",
    "        model = SARIMAX(\n",
    "            train_data,\n",
    "            order=order,\n",
    "            seasonal_order=seasonal_order,\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        \n",
    "        self.sarimax_model = model.fit(disp=False)\n",
    "        print(f\"SARIMAX model training completed\")\n",
    "        \n",
    "        # Print model summary\n",
    "        print(self.sarimax_model.summary())\n",
    "        \n",
    "        return self.sarimax_model\n",
    "    \n",
    "    def train_ml_models(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train XGBoost and LightGBM models for time series forecasting.\n",
    "        \"\"\"\n",
    "        print(f\"Training ML models for {self.target_parameter}...\")\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.feature_scaler.fit_transform(X_train)\n",
    "        \n",
    "        # XGBoost model\n",
    "        xgb_params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        self.xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "        self.xgb_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # LightGBM model\n",
    "        lgb_params = {\n",
    "            'objective': 'regression',\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        self.lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "        self.lgb_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Get feature importances from XGBoost\n",
    "        feature_importances = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance': self.xgb_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 5 important features from XGBoost:\")\n",
    "        print(feature_importances.head())\n",
    "        \n",
    "        return {'xgb': self.xgb_model, 'lgb': self.lgb_model}\n",
    "    \n",
    "    def predict_sarimax(self, steps=24):\n",
    "        \"\"\"\n",
    "        Generate forecasts using the SARIMAX model.\n",
    "        \"\"\"\n",
    "        return self.sarimax_model.forecast(steps=steps)\n",
    "    \n",
    "    def predict_ml_models(self, X_test):\n",
    "        \"\"\"\n",
    "        Generate predictions using ML models.\n",
    "        \"\"\"\n",
    "        X_test_scaled = self.feature_scaler.transform(X_test)\n",
    "        \n",
    "        xgb_pred = self.xgb_model.predict(X_test_scaled)\n",
    "        lgb_pred = self.lgb_model.predict(X_test_scaled)\n",
    "        \n",
    "        # Create ensemble prediction (average of both models)\n",
    "        ensemble_pred = (xgb_pred + lgb_pred) / 2\n",
    "        \n",
    "        return {\n",
    "            'xgb': xgb_pred,\n",
    "            'lgb': lgb_pred,\n",
    "            'ensemble': ensemble_pred\n",
    "        }\n",
    "    \n",
    "    def evaluate_forecasts(self, y_true, y_pred, model_name):\n",
    "        \"\"\"\n",
    "        Evaluate time series forecasts with specific metrics.\n",
    "        \"\"\"\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        \n",
    "        # Calculate MAPE, handling zeros carefully\n",
    "        non_zero_mask = y_true != 0\n",
    "        mape = np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "        \n",
    "        # Time series specific metrics\n",
    "        # Calculate autocorrelation of residuals\n",
    "        residuals = y_true - y_pred\n",
    "        acf_1 = sm.tsa.acf(residuals, nlags=1)[1]  # First-order autocorrelation\n",
    "        \n",
    "        # Durbin-Watson test (values close to 2 indicate no autocorrelation)\n",
    "        dw_stat = sm.stats.stattools.durbin_watson(residuals)\n",
    "        \n",
    "        # Normality test of residuals\n",
    "        _, p_value = stats.shapiro(residuals)\n",
    "        \n",
    "        print(f\"\\nEvaluation metrics for {model_name}:\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"MAPE: {mape:.4f}%\")\n",
    "        print(f\"Autocorrelation (lag 1): {acf_1:.4f}\")\n",
    "        print(f\"Durbin-Watson: {dw_stat:.4f}\")\n",
    "        print(f\"Shapiro-Wilk p-value: {p_value:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'acf_1': acf_1,\n",
    "            'dw_stat': dw_stat,\n",
    "            'residuals_normality_p': p_value\n",
    "        }\n",
    "    \n",
    "    def plot_forecast_vs_actual(self, y_true, y_pred, timestamps, model_name):\n",
    "        \"\"\"\n",
    "        Plot forecast vs actual values.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(timestamps, y_true, label='Actual', marker='o')\n",
    "        plt.plot(timestamps, y_pred, label=f'{model_name} Forecast', marker='x')\n",
    "        plt.title(f'{self.target_parameter} - Actual vs {model_name} Forecast')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel(f'{self.target_parameter} Value')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot residuals\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        residuals = y_true - y_pred\n",
    "        plt.plot(timestamps, residuals, marker='o')\n",
    "        plt.axhline(y=0, color='r', linestyle='-')\n",
    "        plt.title(f'{model_name} Forecast Residuals')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('Residuals')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot residuals distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(residuals, kde=True)\n",
    "        plt.title(f'{model_name} Residuals Distribution')\n",
    "        plt.xlabel('Residual Value')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot autocorrelation of residuals\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sm.graphics.tsa.plot_acf(residuals, lags=24, alpha=0.05)\n",
    "        plt.title(f'Autocorrelation of {model_name} Residuals')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Perform traditional model calibration\n",
    "def prepare_training_data(df):\n",
    "    # Include weather parameters as in notebook 1\n",
    "    features = df[[\n",
    "        'pm25_sensor', 'pm10_sensor', 'o3_sensor', \n",
    "        'co_sensor', 'no2_sensor', 'temperature', 'humidity',\n",
    "        'wind_direction', 'wind_speed', 'precipitation'\n",
    "    ]]\n",
    "    \n",
    "    targets = df[[\n",
    "        'pm25_reference', 'pm10_reference', 'o3_reference',\n",
    "        'co_reference', 'no2_reference'\n",
    "    ]]\n",
    "    \n",
    "    return train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Starting traditional model calibration...\")\n",
    "X_train, X_test, y_train, y_test = prepare_training_data(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train traditional calibration models\n",
    "calibrator = AirQualityCalibrator()\n",
    "calib_results = {}\n",
    "feature_importance_dict = {}\n",
    "\n",
    "for param in calibrator.parameters:\n",
    "    # Train\n",
    "    feature_importance = calibrator.train(\n",
    "        X_train, \n",
    "        y_train[f'{param}_reference'],\n",
    "        param\n",
    "    )\n",
    "    \n",
    "    feature_importance_dict[param] = feature_importance\n",
    "    \n",
    "    # Evaluate\n",
    "    calib_results[param] = calibrator.evaluate(\n",
    "        X_test,\n",
    "        y_test[f'{param}_reference'],\n",
    "        param\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot calibration results\n",
    "def plot_calibration_results(results, parameters):\n",
    "    # Set up the figure\n",
    "    n_cols = 3\n",
    "    n_rows = (len(parameters) + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    axes = axes.flatten() if hasattr(axes, 'flatten') else [axes]\n",
    "    \n",
    "    for idx, param in enumerate(parameters):\n",
    "        if idx < len(axes):\n",
    "            y_true = y_test[f'{param}_reference']\n",
    "            y_pred = results[param]['predictions']\n",
    "            \n",
    "            # Scatter plot\n",
    "            sns.scatterplot(x=y_true, y=y_pred, ax=axes[idx], alpha=0.5)\n",
    "            \n",
    "            # Add perfect prediction line\n",
    "            min_val = min(y_true.min(), y_pred.min())\n",
    "            max_val = max(y_true.max(), y_pred.max())\n",
    "            axes[idx].plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "            \n",
    "            # Add metrics to the title\n",
    "            axes[idx].set_title(f'{param} Calibration\\nRMSE: {results[param][\"rmse\"]:.2f}\\nR²: {results[param][\"r2\"]:.2f}\\nMAE: {results[param][\"mae\"]:.2f}')\n",
    "            axes[idx].set_xlabel('True Values')\n",
    "            axes[idx].set_ylabel('Predicted Values')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for idx in range(len(parameters), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nPlotting calibration results...\")\n",
    "plot_calibration_results(calib_results, calibrator.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "def plot_feature_importances(feature_importance_dict, parameters):\n",
    "    n_cols = 3\n",
    "    n_rows = (len(parameters) + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    axes = axes.flatten() if hasattr(axes, 'flatten') else [axes]\n",
    "    \n",
    "    for idx, param in enumerate(parameters):\n",
    "        if idx < len(axes):\n",
    "            # Get top 10 features\n",
    "            top_features = feature_importance_dict[param].head(10)\n",
    "            \n",
    "            # Create horizontal bar plot\n",
    "            sns.barplot(\n",
    "                data=top_features, \n",
    "                y='feature', \n",
    "                x='importance', \n",
    "                ax=axes[idx]\n",
    "            )\n",
    "            \n",
    "            axes[idx].set_title(f'Top 10 Features for {param}')\n",
    "            axes[idx].set_xlabel('Importance')\n",
    "            axes[idx].set_ylabel('Feature')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for idx in range(len(parameters), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nPlotting feature importances...\")\n",
    "plot_feature_importances(feature_importance_dict, calibrator.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Time Series Forecasting - Focus on PM2.5 (most important pollutant)\n",
    "print(\"\\nStarting time series forecasting for PM2.5...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup time series forecaster\n",
    "ts_forecaster = TimeSeriesForecaster(target_parameter='pm25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare time series data\n",
    "# Convert timestamp columns in splits_info to datetime\n",
    "for col in ['train_start', 'train_end', 'test_start', 'test_end']:\n",
    "    if col in splits_info.columns:\n",
    "        splits_info[col] = pd.to_datetime(splits_info[col])\n",
    "\n",
    "# Get the last split for demonstration\n",
    "last_split = splits_info.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create train and test sets based on the last split\n",
    "train_data = processed_data[\n",
    "    (processed_data['timestamp'] >= last_split['train_start']) & \n",
    "    (processed_data['timestamp'] <= last_split['train_end'])\n",
    "]\n",
    "\n",
    "test_data = processed_data[\n",
    "    (processed_data['timestamp'] >= last_split['test_start']) & \n",
    "    (processed_data['timestamp'] <= last_split['test_end'])\n",
    "]\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train SARIMAX model\n",
    "# First check stationarity\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Apply ADF test on PM2.5 data\n",
    "print(\"\\nChecking stationarity of PM2.5 time series...\")\n",
    "adf_result = adfuller(train_data['pm25_sensor'].dropna())\n",
    "print(f'ADF Statistic: {adf_result[0]}')\n",
    "print(f'p-value: {adf_result[1]}')\n",
    "print('Critical Values:')\n",
    "for key, value in adf_result[4].items():\n",
    "    print(f'\\t{key}: {value}')\n",
    "\n",
    "# If not stationary, we might want to take differences\n",
    "if adf_result[1] > 0.05:\n",
    "    print(\"Series is not stationary. Consider differencing in SARIMAX.\")\n",
    "else:\n",
    "    print(\"Series is stationary.\")\n",
    "\n",
    "# Train SARIMAX model on univariate PM2.5 data\n",
    "print(\"\\nTraining SARIMAX model...\")\n",
    "# Use a smaller subset for SARIMAX due to computational complexity\n",
    "sarimax_train = train_data['pm25_sensor'].iloc[-7*24:]  # Last 7 days\n",
    "sarimax_model = ts_forecaster.train_sarimax(\n",
    "    sarimax_train, \n",
    "    order=(1, 1, 1),  # ARIMA component (p, d, q)\n",
    "    seasonal_order=(1, 1, 1, 24)  # Seasonal component (P, D, Q, s) with 24-hour seasonality\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Forecast using SARIMAX\n",
    "n_steps = len(test_data)\n",
    "print(f\"\\nForecasting {n_steps} steps ahead...\")\n",
    "sarimax_forecast = ts_forecaster.predict_sarimax(steps=n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate SARIMAX forecast\n",
    "# Make sure we have the same number of test points as forecast points\n",
    "actual_values = test_data['pm25_sensor'].iloc[:len(sarimax_forecast)]\n",
    "timestamps = test_data['timestamp'].iloc[:len(sarimax_forecast)]\n",
    "\n",
    "print(\"\\nEvaluating SARIMAX forecast...\")\n",
    "sarimax_metrics = ts_forecaster.evaluate_forecasts(\n",
    "    actual_values.values, \n",
    "    sarimax_forecast, \n",
    "    'SARIMAX'\n",
    ")\n",
    "\n",
    "# Plot SARIMAX results\n",
    "ts_forecaster.plot_forecast_vs_actual(\n",
    "    actual_values.values,\n",
    "    sarimax_forecast,\n",
    "    timestamps,\n",
    "    'SARIMAX'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save Models\n",
    "def save_models(calibrator, ts_forecaster, bucket_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    for param, model in calibrator.models.items():\n",
    "        # Save locally first\n",
    "        local_path = f'/tmp/{param}_model.joblib'\n",
    "        joblib.dump(model, local_path)\n",
    "        \n",
    "        # Upload to GCS\n",
    "        blob = bucket.blob(f'models/{param}_model.joblib')\n",
    "        blob.upload_from_filename(local_path)\n",
    "        \n",
    "        print(f\"Saved model for {param}\")\n",
    "\n",
    "save_models(calibrator, BUCKET)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
